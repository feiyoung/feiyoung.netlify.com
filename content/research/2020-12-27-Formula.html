---
title: "Common-used Formula"
author: "Wei Liu"
date: '2020-12-27'
output: pdf_document
layout: post
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="computational-complexity-of-matrix" class="section level2">
<h2>Computational Complexity of Matrix</h2>
<p>We list the common-used complexity.</p>
<ol style="list-style-type: decimal">
<li><p>The complexity of computing <span class="math inline">\(|A|\)</span> is <span class="math inline">\(O(n^3)\)</span> if <span class="math inline">\(A \in R^{n\times n}\)</span>.</p></li>
<li><p>The complexity of computing <span class="math inline">\(A^{-1}\)</span> is <span class="math inline">\(O(n^3)\)</span> if <span class="math inline">\(A \in R^{n\times n}\)</span>.</p></li>
<li><p>The complexity of <span class="math inline">\(AB\)</span> is <span class="math inline">\(O(nmp)\)</span> if <span class="math inline">\(A \in R^{n\times m}, B \in R^{m \times p}\)</span>.</p></li>
<li><p>The complexity of Singular value decomposition of <span class="math inline">\(A\)</span> is <span class="math inline">\(O(mn^2)\)</span> if <span class="math inline">\(A \in R^{m\times n}\)</span>, where <span class="math inline">\(n\geq m\)</span>.</p></li>
<li><p>The complexity of QR decomposition of <span class="math inline">\(A\)</span> is <span class="math inline">\(O(\min(mn^2,nm^2))\)</span> if <span class="math inline">\(A \in R^{m\times n}\)</span>.</p></li>
</ol>
</div>
<div id="stirlings-formula" class="section level2">
<h2>Stirling’s Formula</h2>
<p>This formula transfers the factorial term into a polynomial term.
The formula is given by
<span class="math display">\[n! = (\frac{n}{e})^n \sqrt{2\pi n}.\]</span></p>
<pre class="rmd"><code>References:
James Stirling, 1970, Differential Method with a Tract on Summation and Interpolation of Infinite Series.</code></pre>
</div>
<div id="shermanmorrisonwoodbury-formula" class="section level2">
<h2>Sherman–Morrison–Woodbury Formula</h2>
<p>This formula is commonly used for statistical computing, which transfers a high-dimensional matrix inverse into a low-dimensional matrix inverse.</p>
<p>Assume <span class="math inline">\(A \in R^{p \times p}, U \in R^{p\times n}, B \in R^{n \times n}, V \in R^{p \times n}\)</span>, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are invertible, then
<span class="math display">\[(A + UBV^T)^{-1}=A^{-1} - A^{-1} U(B^{-1} + V^T A^{-1} U)^{-1} V^T A^{-1}.  \tag{2.1}\]</span>
See <a href="http://fourier.eng.hmc.edu/e176/lectures/algebra/node6.html" class="uri">http://fourier.eng.hmc.edu/e176/lectures/algebra/node6.html</a> for the detail of proofs.</p>
<p>Special Case I: In statistics, we care about <span class="math inline">\(R=(\lambda I_p + X^T X)^{-1}\)</span>, where <span class="math inline">\(X \in R^{n \times p}\)</span>. We know <span class="math inline">\(R = \lambda^{-1} (I_p + X^T \lambda^{-1} I_n X)^{-1}\)</span>
Let <span class="math inline">\(A=I_p, U=V=X^T, B =\lambda^{-1} I_n\)</span>. By <span class="math inline">\((2.1)\)</span>, we can easily obtain
<span class="math display">\[R = \lambda^{-1} \{I_p - X^T(\lambda I_n + X X^T)^{-1} X\}.\]</span></p>
</div>
<div id="matrix-multiplication-formula" class="section level2">
<h2>Matrix multiplication formula</h2>
<p>Vector calculation plays an important role in statistical computing.</p>
<p>Suppose <span class="math inline">\(A \in R^{I \times J}, B \in R^{K \times L}, C \in R^{J \times M}, D \in R^{L \times N}\)</span>，then
<span class="math display">\[(A \otimes B) (C \otimes D)=(AC) \otimes (BD), \]</span>
<span class="math display">\[(A \otimes B)^+= A^+ \otimes B ^+, (A \otimes B)^T= A^T \otimes B ^T,\]</span>
<span class="math display">\[A \odot B \odot C = (A \odot B) \odot C = A \odot (B \odot C)\]</span>
<span class="math display">\[(A \odot B)^T (A \odot B) = A^TA * B^TB,A \in R^{I \times J}, B \in R^{K \times J}\]</span>
<span class="math display">\[(A \odot B)^+= (A^TA * B^TB)^+ (A \odot B)^T,\]</span>
其中A+表示Moore-Penrose伪逆。</p>
<p>矩阵乘积向量化换序公式：
<span class="math display">\[vec(AXB) = (B^T \otimes A) vec (X). \tag{A1}\]</span></p>
<p>特例1：令B=b为列向量，则
<span class="math display">\[AXb= (b^T \otimes A)vec(X) \tag{A3}\]</span></p>
<p>特例2：若<span class="math inline">\(X \in R^{I \times J}, b\in R^{J\times 1}\)</span>，则有
<span class="math display">\[Xb = (b^T \otimes I_I) vec(X) \in R^I, \tag{A2}\]</span>
其中II表示<span class="math inline">\(I \times I的单位矩阵。由(A1)可以直接推导出(A2)，因为Xb = I_I Xb\)</span>，列向量是一种特殊的矩阵。</p>
<p>设<span class="math inline">\(A\in R^{K\times I}, B \in R^{I \times m}, C \in R^{m \times n}，于是基于(A1)\)</span>，可以推导出常用的矩阵向量化边缘轮换公式：
<span class="math display">\[vec(ABC) = (I_n \otimes AB) vec(C)=(C^T B^T \otimes I_K) vec(A) \tag{A4}\]</span>
注意：第一个等式将矩阵C的向量化版本轮换到最右方的边缘；第二个等式将矩阵A的向量化版本轮换到最右方的边缘。在统计中这样做的好处是：如果A或C是我们感兴趣的参数，则(A4)将参数和数据分离，帮助我们推导参数的显示表达式，或者迭代的显式表达。</p>
<p>(A4)的特例：
<span class="math display">\[vec(AB) = (I_m \otimes A) vec(B) = (B^T \otimes I_K) vec(A)  \tag{A5}\]</span>
利用(A5)，我们可以得到
<span class="math display">\[vec(ABC)= (I_n \otimes A) (C^T \otimes I_I) vec(B)  \tag{A6}\]</span>
(A6)告诉我们如何将夹在矩阵中间的参数轮换到边上去。</p>
<p>其他公式：1）Hardamard乘积：<span class="math inline">\(A, B \in R^{n \times m}\)</span>，
vec(A∗B)=vec(A)∗vec(B)</p>
<p>2)Inner product（内积）：<span class="math inline">\(A, B \in R^{n \times m}\)</span>，
<span class="math display">\[\langle A, B\rangle = tr(A^TB)=vec(A)^T vec(B)=vec(B^T)^T vec(A)\]</span></p>
</div>
<div id="common-used-matrixvector-derivative-formula" class="section level2">
<h2>Common-used matrix(vector) derivative formula</h2>
<p>As mentioned above, there are two common-used laying-out systems of partial derivatives in vectors and matrices, and no standard appears to be emerging yet. In general, the numerator layout convention is often for the purposes of convenience.
It is claimed that a vector is layouted in column vector form as default.</p>
<ol style="list-style-type: decimal">
<li><p>It is worthwhile to note that <span class="math inline">\(y=F(w), y\in R, w\in R^q\)</span>, then
<span class="math display">\[\frac{\partial y}{\partial w}= (\frac{\partial y}{\partial w_1}, \cdots, \frac{\partial y}{\partial w_q}) \in R^{1\times q}.\]</span></p></li>
<li><p>A scalar making derivative on a column vector produces a row vector. But a column vector funciton making derivative on a scalar produces a column vector, e.g. 
<span class="math display">\[\frac{\partial w}{\partial y}= (\frac{\partial w_1}{\partial y}, \cdots, \frac{\partial w_q}{\partial y})^T \in R^{q \times 1}.\]</span></p></li>
<li><p>The derivative of a vector function (a vector whose components are functions) <span class="math inline">\(y\in R^p\)</span> with respect to an input vector <span class="math inline">\(x\in R^q\)</span> is written (in numerator layout notation) as
<span class="math display">\[\frac{\partial y}{\partial x}=[\frac{\partial y_1}{\partial x_1}, \cdots, \frac{\partial w_p}{\partial x_1}; \cdots; \frac{\partial y_1}{\partial x_q}, \cdots, \frac{\partial y_q}{\partial x_q}]^T \in R^{p\times q},\]</span>
whose each column is <span class="math inline">\(\frac{\partial y}{\partial x_j} \in R^{p \times 1}\)</span>,where the notation mimics the matrix in Matlab.</p></li>
<li><p>The derivative of a scalar function <span class="math inline">\(y\in R\)</span> w.r.t. an matrix <span class="math inline">\(X\in R^{p\times q}\)</span> is written as
<span class="math display">\[\frac{\partial y}{\partial X}=[\frac{\partial y}{\partial x_{11}}, \cdots, \frac{\partial y}{\partial x_{p1}}; \cdots; \frac{\partial y}{\partial x_{1q}}, \cdots, \frac{\partial y}{\partial x_{pq}}] \in R^{q\times p}\]</span></p></li>
<li><p>The derivative of a matrix function <span class="math inline">\(Y\in R^{p\times q}\)</span> w.r.t. a scalar <span class="math inline">\(x\in R\)</span> is known as tangent matrix, and written as
<span class="math display">\[\frac{\partial Y}{\partial x}=[\frac{\partial y_{11}}{\partial x}, \cdots, \frac{\partial y_{1q}}{\partial x}; \cdots; \frac{\partial y_{p1}}{\partial x}, \cdots, \frac{\partial y_{pq}}{\partial x}] \in R^{p\times q}\]</span></p></li>
</ol>
<p>In a word, the size of a derivative is dimension of dependent variables times that of transpose of independent variables, i.e., dim(dy/dx)=dim(dim(y) dim(x^T)). For example, <span class="math inline">\(y\in R, X\in R^{p\times q}\)</span>, then <span class="math inline">\(dim(dy/dX)= dim(1\times (q\times p))=q\times p\)</span>.</p>
<p>(1)Scalar-to-Matrix derivative: let <span class="math inline">\(W\in R^{p\times q}\)</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F(W)= a^TWb: \frac{\partial F}{\partial W}= b a^T;\)</span></p></li>
<li><p><span class="math inline">\(F(W)= a^TW^Tb: \frac{\partial F}{\partial W}= a b^T;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=a(W)b(W):\frac{\partial F}{\partial W}=a(W)\frac{\partial b(W)}{\partial W}+ b(W)\frac{\partial a(W)}{\partial W};\)</span>;</p></li>
<li><p>Trace funtion is only welll-defined for square matrix, so if <span class="math inline">\(p=q\)</span>,
<span class="math inline">\(F(W)=tr(W): \frac{\partial F}{\partial W}= I_q;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(U(W)+ V(W)): \frac{\partial F}{\partial W}= \frac{\partial tr(U)}{\partial W}+\frac{\partial tr(V)}{\partial W};\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(AW)=tr(WA): \frac{\partial F}{\partial W}= A;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(AW^T)=tr(W^TA): \frac{\partial F}{\partial W}= A^T;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(W^TAW): \frac{\partial F}{\partial W}= W^T(A+A^T);\)</span>
I found that we can use the LRD(Left and Right) Derivative method to obtain the result, i.e. <span class="math inline">\(\frac{\partial tr(W^TAW)}{\partial W}= \frac{\partial tr((W^TA)W)}{\partial W}+ \frac{\partial tr(W^T(AW))}{\partial W}=W^TA + W^T A^T\)</span>, where <span class="math inline">\((\cdot)\)</span> is regard as constant.</p></li>
<li><p><span class="math inline">\(F(W)=tr(W^{-1}A): \frac{\partial F}{\partial W}= -W^{-1}AW^{-1};\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(AWB)=tr(WBA): \frac{\partial F}{\partial W}= BA;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=tr(AWBW^TC)=tr(CAWBW^T): \frac{\partial F}{\partial W}= (CAWB)^T+ BW^TCA;\)</span> This can be obtained by LRD method.</p></li>
<li><p><span class="math inline">\(F(W)=|W|: \frac{\partial F}{\partial W}= |W|W^{-1};\)</span></p></li>
<li><p><span class="math inline">\(F(W)=\ln(a|W|): \frac{\partial F}{\partial W}= W^{-1};\)</span></p></li>
<li><p><span class="math inline">\(F(W)=\|Wa +b\|^2: \frac{\partial F}{\partial W}= 2a (Wa+b)^T;\)</span></p></li>
<li><p><span class="math inline">\(F(W)=(Wa)^T C W b: \frac{\partial F}{\partial W}= ab^TW^TC^T + b a^T W^TC;\)</span> This can be otained by 1), 2) and LRD method.</p></li>
</ol>
<p>I conclude 1) the Layout-Match (LM) method: <span class="math inline">\(nrow(\frac{\partial F}{\partial W})=ncol(W)\)</span> to check the correction of dimension of results; 2) LRD method can help to deduce the derivative of multiplication terms.</p>
<p>See <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors" class="uri">https://en.wikipedia.org/wiki/Matrix_calculus#Derivatives_with_vectors</a> for More information.</p>
</div>
