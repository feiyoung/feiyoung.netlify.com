---
title: "EM algorithm for mixture probabilistic PCA"
author: "Wei Liu"
date: '2021-01-15'
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<div id="mixture-probabilistic-pca" class="section level1">
<h1>mixture probabilistic PCA</h1>
<p>Consider a model
<span class="math display">\[x_i = \mu_k + W_k z_i + \varepsilon_k, ~~ if~y_{ik}=1,y_{ij}=0,i\neq j.\tag{1.1}\]</span>
where <span class="math inline">\(x_i \in R^p, z_i \in R^q, \varepsilon_k\sim N(0, \sigma^2_kI_p)\)</span>,<span class="math inline">\(z_i \sim N(0, I_q), W_k \in R^{p\times q}, y_i \sim Multinormial(1,\pi),\pi \in R^K\)</span>.
First, it performs clustering, then conducts PCA.</p>
<p>By <span class="math inline">\((1.1)\)</span>, we can obtain the complete-data likelihood for individual <span class="math inline">\(i\)</span>,
<span class="math inline">\(P(x_i,z_i, y_i)=p(x_i|z_i,y_i)p(z_i|y_i)p(y_i)\)</span>, whose specific form is
<span class="math display">\[\Pi_{k=1}^K\{\pi_k f_{ki} g_i\}^{y_{ik}},\]</span>
where
<span class="math display">\[f_{ki}=(2\pi \sigma_k^2)^{-p/2}\exp\{\frac{-1}{2\sigma_k^2}\|x_i-\mu_k-W_kz_i\|^2\}, g_i=(2\pi)^{-q/2}\exp(-\frac{1}{2}\|z_i\|^2).\]</span>
The corresponding loglikelihood is given by
<span class="math display">\[l=\sum_{k=1}^K y_{ik} \{\ln(\pi_k) + \ln f_{ki} + \ln g_i\} \tag{1.2}\]</span>
# EM algorithm
To deduce EM algorithm, we first calculate the posterior distribution of <span class="math inline">\((y_i, z_i)\)</span> given <span class="math inline">\(x_i\)</span> and parameters by previous iteration. Noting each <span class="math inline">\(y_{ik}\)</span> is seperatable, we consider
<span class="math display">\[P(y_{ik}=1, z_i | x_i)=P(y_{ik}=1|x_i)P(z_i|y_{ik}=1,x_i)=R_{ik}f_{k}(z_i),\]</span>
where <span class="math inline">\(f_k(z_i)\)</span> is the posterior distribution of <span class="math inline">\(z_i\)</span> given <span class="math inline">\(x_i\)</span> when <span class="math inline">\(y_{ik}=1\)</span>.
By <span class="math inline">\((8)\)</span> in Bishop (1999), we have
<span class="math display">\[f_{k}(z_i)=(2\pi)^{-q/2}|\sigma_k^2M_k|^{1/2}\exp\{(z_i-s_k(x_i)^T\sigma_k^{-2}M_k(z_i -s_k(x_i))\},\]</span>
where <span class="math inline">\(\sigma_k^2M_k^{-1}=\sigma_k^2(\sigma^2_kI_q+W_k^TW_k)^{-1}\)</span> and <span class="math inline">\(s_k(x_i)=M_k^{-1}W_k(x_i-\mu_k)\)</span>.</p>
<div id="e-step" class="section level2">
<h2>E-step</h2>
<p>We rewrite <span class="math inline">\((1.2)\)</span> as the specific form,
<span class="math display">\[l_k=y_{ik}\{\ln(\pi_k) - \frac{p}{2} \ln(2\pi \sigma^2_k) - \frac{1}{2\sigma_k^2}\|x_i-\mu_k-W_kz_i\|^2-\frac{q}{2} \ln(2\pi)-\frac{1}{2}\|z_i\|^2\}.\]</span>
Omitting the terms independent of parameters, we have
<span class="math display">\[l_k = y_{ik}\{\ln(\pi_k) - \frac{p}{2} \ln(\sigma^2_k) - \frac{1}{2\sigma_k^2}\|x_i-\mu_k-W_kz_i\|^2-\frac{1}{2}\|z_i\|^2\}.\]</span>
Thus
<span class="math display">\[E_{(y_{ik},z_i)|x_i}(l_k)=\int l_k(y_{ik}, z_i)P(y_{ik}=1,z_i|x_i)d(y_{ik},z_i)=\int l_k(1,z_i)R_{ik}f_{k}(z_i)dz_i.\tag{1.3}\]</span>
Denote <span class="math inline">\(h_k(z_i)=\ln(\pi_k) - \frac{p}{2} \ln(\sigma^2_k) - \frac{1}{2\sigma_k^2}\|x_i-\mu_k-W_kz_i\|^2-\frac{1}{2}\|z_i\|^2\)</span><span class="math inline">\(=\ln(\pi_k) - \frac{p}{2} \ln(\sigma^2_k)-\frac{1}{2}z_i^Tz_i - \frac{1}{2\sigma_k^2}\{z_i^TW^TWz_i-2(x_i-\mu_k)^TW_kz_i+ \|x_i-\mu_k\|^2\}.\)</span>
Furthermore, <span class="math inline">\((1.3)\)</span> simplifies as
<span class="math display">\[E_{(y_{ik},z_i)|x_i}(l_k)= R_{ik} \int h_k(z_i) f_{k}(z_i)dz_i. \tag{1.4}\]</span>
<span class="math inline">\((1.4)\)</span> only involves the posterior first-order moment and second-order moment of <span class="math inline">\(z_i\)</span> that are denoted by
<span class="math display">\[\langle z_i\rangle=M_i^{-1}W_i^T(x_i-\mu_k)\]</span>
and
<span class="math display">\[\langle z_iz_i^T\rangle= \sigma^2_kM_i^{-1} +\langle z_i\rangle\langle z_i\rangle^T. \]</span>
Similar to (54) in Bishop (1999), we obtain
<span class="math display">\[E_{(y_i,z_i)|x_i}l = \sum_{k=1}^K E_{(y_{ik},z_i)|x_i}(l_k)= \sum_{k=1}^K R_{ik}\{\ln(\pi_k) - \frac{p}{2} \ln(\sigma^2_k) -\frac{1}{2}\langle z_iz_i^T\rangle - \frac{1}{2\sigma_k^2}(tr(W_k^TW_k\langle z_iz_i^T\rangle)-2(x_i-\mu_k)^TW_k \langle z_i \rangle+\|x_i-\mu_k\|^2)\}.\]</span>
Finally, we obtain the Q-function,
<span class="math display">\[Q(\theta;\theta^{(t)})=\sum\limits_{i=1}^n\sum\limits_{k=1}^KR_{ik}(\theta^{(t)})\{\ln(\pi_k) - \frac{p}{2} \ln(\sigma^2_k) -\frac{1}{2}\langle z_iz_i^T\rangle - \frac{1}{2\sigma_k^2}(tr(W_k^TW_k\langle z_iz_i^T\rangle)-2(x_i-\mu_k)^TW_k \langle z_i \rangle+\|x_i-\mu_k\|^2)\},\]</span>
where <span class="math inline">\(\langle z_i\rangle\)</span> and <span class="math inline">\(\langle z_iz_i^T\rangle\)</span> also include <span class="math inline">\(\theta^{(t)}\)</span>.</p>
</div>
<div id="m-step" class="section level2">
<h2>M-step</h2>
<p>This step is to maximize the Q-function. Since the constraint <span class="math inline">\(\sum_{k=1}^K \pi_k=1\)</span> is required, we use Langrange method to obtain a new objective function,
<span class="math display">\[L(\theta, \lambda;\theta^{(t)})=Q(\theta;\theta^{(t)}) + \lambda (1-\sum_{k=1}^K \pi_k).\]</span>
1)Taking derivative on <span class="math inline">\(\pi_k, \lambda\)</span>, and setting it to zero, we obtain
<span class="math display">\[\frac{\partial L}{\partial \pi_k}=\sum\limits_{i=1}^n R_{ik}(\theta^{(t)}) \pi_k^{-1} - \lambda=0 \tag{2.2.1}\]</span>
<span class="math display">\[\sum_{k=1}^K \pi_k=1 \tag{2.2.2}\]</span>
Combining <span class="math inline">\((2.2.1)\)</span> and <span class="math inline">\((2.2.2)\)</span>, we conclude
<span class="math display">\[\pi_k^{(t+1)} = n^{-1}\sum\limits_{i=1}^n R_{ik}(\theta^{(t)})\]</span>
by using the fact that <span class="math inline">\(\sum\limits_{i=1}^n(\sum\limits_{k=1}^KR_{ik}(\theta^{(t)}))=n.\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Taking derivative on <span class="math inline">\(\mu_k\)</span>, we have
<span class="math display">\[\mu_k^{(t+1)}=\frac{\sum\limits_{i=1}^n R_{ik}(\theta^{(t)})\{x_i - W_k^{(t+1)}\langle z_i\rangle^{(t)}\}}{\sum\limits_{i=1}^n R_{ik}(\theta^{(t)})}\]</span></li>
<li>Taking derivative on <span class="math inline">\(W_k\)</span>, we get
<span class="math display">\[W^{(t+1)}\]</span></li>
</ol>
</div>
<div id="common-used-matrixvector-derivative-formula" class="section level2">
<h2>Common-used matrix(vector) derivative formula</h2>
<p>It is claimed that a vector is layouted in column vector form as default.</p>
<p>1)It is worthwhile to note that <span class="math inline">\(y=F(w), y\in R, w\in R^q\)</span>, then
<span class="math display">\[\frac{\partial y}{\partial w}= (\frac{\partial y}{\partial w_1}, \cdots, \frac{\partial y}{\partial w_q}) \in R^{1\times q}.\]</span></p>
<p>2)A scalar making derivative on a column vector produces a row vector. But a column vector funciton making derivative on a scalar produces a column vector, e.g.Â 
<span class="math display">\[\frac{\partial w}{\partial y}= (\frac{\partial w_1}{\partial y}, \cdots, \frac{\partial w_q}{\partial y})^T \in R^{q \times 1}.\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p>The derivative of a vector function (a vector whose components are functions) <span class="math inline">\(y\in R^p\)</span> with respect to an input vector <span class="math inline">\(x\in R^q\)</span> is written (in numerator layout notation) as
<span class="math display">\[\frac{\partial y}{\partial x}=[\frac{\partial y_1}{\partial x_1}, \cdots, \frac{\partial w_p}{\partial x_1}; \cdots; \frac{\partial y_1}{\partial x_q}, \cdots, \frac{\partial y_q}{\partial x_q}]^T \in R^{p\times q},\]</span>
whose each column is <span class="math inline">\(\frac{\partial y}{\partial x_j} \in R^{p \times 1}\)</span>,where the notation mimics the matrix in Matlab.</p></li>
<li><p>The derivative of a scalar function <span class="math inline">\(y\in R\)</span> w.r.t. an matrix <span class="math inline">\(X\in R^{p\times q}\)</span> is written as
<span class="math display">\[\frac{\partial y}{\partial X}=[\frac{\partial y}{\partial x_{11}}, \cdots, \frac{\partial y}{\partial x_{p1}}; \cdots; \frac{\partial y}{\partial x_{1q}}, \cdots, \frac{\partial y}{\partial x_{pq}}] \in R^{q\times p}\]</span></p></li>
<li><p>The derivative of a matrix function <span class="math inline">\(Y\in R^{p\times q}\)</span> w.r.t. an matrix <span class="math inline">\(x\in R\)</span> is known as tangent matrix, and written as
<span class="math display">\[\frac{\partial Y}{\partial x}=[\frac{\partial y_{11}}{\partial x}, \cdots, \frac{\partial x_{p1}}{\partial x}; \cdots; \frac{\partial x_{1q}}{\partial x}, \cdots, \frac{\partial y_{pq}}{\partial x}] \in R^{q\times p}\]</span></p></li>
</ol>
<p>In a word, the size of a derivative is dimension of dependent variables times that of transpose of independent variables, i.e., dim(dy/dx)=dim(dim(y) dim(x^T)). For example, <span class="math inline">\(y\in R, X\in R^{p\times q}\)</span>, then <span class="math inline">\(dim(dy/dX)= dim(1\times (q\times p))=q\times p\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Scalar-to-Matrix derivative: let <span class="math inline">\(W\in R^{p\times q}\)</span></li>
</ol>
<p>F(W)=tr(W): <span class="math inline">\(\frac{\partial F}{\partial W}= I_q ~if~ p=q\)</span>;</p>
<p>F(W)= a^TWb: <span class="math inline">\(\frac{\partial F}{\partial W}= b a^T\)</span>;</p>
<p>F(W)= a<sup>TW</sup>Tb: <span class="math inline">\(\frac{\partial F}{\partial W}= a b^T\)</span>;</p>
</div>
</div>
